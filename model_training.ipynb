{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, roc_curve,roc_auc_score,auc, f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape des données du Train.csv : (78074, 689)\n",
      "shape des données de Test.csv : (38992, 689)\n"
     ]
    }
   ],
   "source": [
    "#formatage des données de training\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('Train_transformed.csv',  sep=';')\n",
    "\n",
    "df_submission_test = pd.read_csv('Test_submission_transformed.csv', sep=';')\n",
    "\n",
    "print(\"shape des données du Train.csv : {}\".format(df_train.shape))\n",
    "print(\"shape des données de Test.csv : {}\".format(df_submission_test.shape))\n",
    "\n",
    "\n",
    "\n",
    "X = df_train.drop('return90',axis=1)\n",
    "y = df_train['return90']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78074, 688)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerem/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78074, 29)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "rf_reduc = RandomForestClassifier()\n",
    "\n",
    "rf_reduc.fit(X, y)\n",
    "\n",
    "model_reduc = SelectFromModel(rf_reduc, prefit=True)\n",
    "\n",
    "X_new = model_reduc.transform(X)\n",
    "\n",
    "print(X_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 110, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#df_sample_1000 = pre_select_sample.sample(n=50000, random_state=20) \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state= 20)\n",
    "\n",
    "\n",
    "# nombre d'arbre\n",
    "n_estimators = [100,200,300]\n",
    "\n",
    "# nombre de features à chaque split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# nombre max d'etage pour chaque arbre\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# nombre mini pour split un noeud\n",
    "min_samples_split = [2, 5, 10]\n",
    "# nombre mini pour chaque feuille\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# methode de selection des sous ensemble du dataset\n",
    "bootstrap = [True, False]\n",
    "\n",
    "#grille pour les test des parametres optimaux\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# 3 folds, 10 iterations = 30 test = env 8min sur le datasets sans reduction de dimensionnalités\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit le model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"début training randomforest\")\n",
    "\n",
    "#clf_rf = RandomForestClassifier(n_estimators = 5,min_samples_split=2, min_samples_leaf = 2, class_weight=\"balanced\")\n",
    "#clf_rf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "#print(\"fin training\")\n",
    "\n",
    "#y_predict_sample = clf_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test,y_predict_sample)\n",
    "#plt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n",
    "\n",
    "\n",
    "#print(\"AUC: {}\".format(roc_auc_score(y_test, y_predict_sample)))\n",
    "\n",
    "\n",
    "#reduction de dimensionnalité\n",
    "#peut etre à utiliser pour un SVC\n",
    "\n",
    "#print(X.shape)\n",
    "#logreg_reduc = LogisticRegression(penalty=\"l1\")\n",
    "#logreg_reduc.fit(X, y)\n",
    "#model_reduc = SelectFromModel(logreg_reduc, prefit=True)\n",
    "#X_new = model_reduc.transform(X)\n",
    "#print(X_new.shape)\n",
    "\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_new,y,test_size=0.3, random_state= 20)\n",
    "#best_p = tree_cv.best_params_\n",
    "#rf = SVC(gamma='auto',probability=True)\n",
    "\n",
    "#rf.fit(X_train,y_train)\n",
    "\n",
    "#y_rf_p = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "#print(y_rf_p)\n",
    "\n",
    "#fpr, tpr, _ = roc_curve(y_test,y_rf_p)\n",
    "#plt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n",
    "\n",
    "\n",
    "#print(\"AUC: {}\".format(roc_auc_score(y_test, y_rf_p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78074, 688)\n",
      "(78074, 42)\n"
     ]
    }
   ],
   "source": [
    "#reduction de dimensionnalité\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "rf_reduc = RandomForestClassifier(n_estimators = 300,min_samples_split=5, min_samples_leaf = 4,\n",
    "                                max_features= 'sqrt', max_depth = 40, bootstrap = False)\n",
    "\n",
    "rf_reduc.fit(X, y)\n",
    "\n",
    "model_reduc = SelectFromModel(rf_reduc, prefit=True)\n",
    "\n",
    "X_new = model_reduc.transform(X)\n",
    "\n",
    "print(X_new.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "début training randomforest\n"
     ]
    }
   ],
   "source": [
    "#random forest avec les parametres tunés\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state= 5)\n",
    "\n",
    "print(\"début training randomforest\")\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators = 300,min_samples_split=5, min_samples_leaf = 4,\n",
    "                                max_features= 'sqrt', max_depth = 110, bootstrap = False, class_weight= \"balanced\")\n",
    "\n",
    "\n",
    "clf_rf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"fin training\")\n",
    "\n",
    "y_predict = clf_rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test,y_predict)\n",
    "plt.plot(fpr, tpr, marker='.', label='LogistiIIc')\n",
    "\n",
    "\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_predict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance du model\n",
    "#peut etre tune le meilleur solver, sachant que lgbs supporte seulement une norme L2\n",
    "#norme L1 ou L2 ?\n",
    "\n",
    "#faire une selection de variable avant de tuné le modele\n",
    "\n",
    "logreg = LogisticRegression(penalty=\"l2\", solver= 'lbfgs')\n",
    "\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n",
    "\n",
    "\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "\n",
    "#Meilleur parametre C\n",
    "bestparam = logreg_cv.best_params_\n",
    "\n",
    "\n",
    "print(\"Parametre C tuné du model: {}\".format(bestparam)) \n",
    "print(\"Meilleur score {}\".format(logreg_cv.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model de la submission 1 sur kaggle\n",
    "\n",
    "clf_logreg = LogisticRegression(penalty=\"l1\")\n",
    "clf_logreg.fit(X,y)\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test,y_pred)\n",
    "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
    "\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "#applique le modèle sur les données de test du Test.csv à envoyé à Kaggle\n",
    "#choper la proba\n",
    "#peut etre revoir le treshold mais comme on veut la proba dans ce cas la ca pose pas trop de pb pour le moment\n",
    "df_X_sub_split_dropped = df_X_sub_split.drop('return90',axis=1)\n",
    "\n",
    "y_pred = clf_logreg.predict_proba(df_X_sub_split_dropped)[:,1]\n",
    "print(y_pred)\n",
    "\n",
    "\n",
    "index = df_X_sub_split_dropped['booking_id'].astype(np.int64)\n",
    "submission_df = pd.DataFrame({'return90': y_pred},index=index)\n",
    "\n",
    "\n",
    "#submission_df.to_csv('submission1.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52608365 0.53618308 0.39018932 ... 0.22911772 0.15339351 0.15448979]\n"
     ]
    }
   ],
   "source": [
    "X_df_sub = df_submission_test.drop('return90', axis = 1)\n",
    "\n",
    "X_df_sub_reduc = X_df_sub.loc[:, model_reduc.get_support()]\n",
    "\n",
    "y_pred_sub = clf_rf.predict_proba(X_df_sub)[:,1]\n",
    "print(y_pred_sub)\n",
    "\n",
    "\n",
    "index = X_df_sub_reduc['booking_id'].astype(np.int64)\n",
    "submission_df = pd.DataFrame({'return90': y_pred_sub},index=index)\n",
    "\n",
    "submission_df.to_csv('submission2.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
